{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50db0c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source', 'id', 'text'],\n",
      "        num_rows: 1392522\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "data = load_dataset(\"artem9k/ai-text-detection-pile\")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f1756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilroberta-base\", num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce348d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f32ff24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152522"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"train\"]) - 1240000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22586f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4da79971d44549b512cad1bbd0fd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_fn(batch):\n",
    "    labels = [1 if src == \"human\" else 0 for src in batch[\"source\"]]\n",
    "    tokenized = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256,\n",
    "    )\n",
    "    tokenized[\"label\"] = labels\n",
    "    return tokenized\n",
    "train_data = data[\"train\"]\n",
    "data_human = train_data.select(range(152522))\n",
    "data_ai = train_data.select(range(1240000, len(train_data)))\n",
    "merged_data = concatenate_datasets([data_human,data_ai])\n",
    "merged_data = merged_data.shuffle(seed=42)\n",
    "preprocessed_dataset = merged_data.map(preprocess_fn, batched=True)\n",
    "preprocessed_dataset = preprocessed_dataset.remove_columns([\"id\",\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6362199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305044"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2450dbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Benign T-wave Inversion from HQMedEd on Vimeo . There are many etiologies of T-wave inversion.\\xa0 We are most worried about ischemic T-wave inversion.\\xa0 Wellens\\' syndrome is particularly dangerous, as it signifies an unstable critical LAD stenosis.\\xa0 I have several posts on this; here is one that shows the entire evolution . Another etiology is \"Benign T-wave Inversion\", which has long been recognized. I first saw it described in Chou\\'s textbook.\\xa0 It is a normal variant associated with early repolarization.\\xa0 K. Wang recently studied it. \\xa0 He reviewed ECGs from all 11,424 patients who had at least one recorded during 2007 at Hennepin County Medical Center (where I work) and set aside the 101 cases of benign T-wave inversion.\\xa0 97 were black.\\xa0 3.7% of black men and\\xa0 1% of black women had this finding.\\xa0 1 of 5099 white patients had it.\\xa0 Aside from an 8.8% incidence (9 of 109) black males aged 17-19, it was evenly distributed by age group. I have reviewed these 101 ECGs, and what strikes me is: 1. There is a relatively short QT interval (QTc < 425ms) 2. The leads with T-wave inversion often have very distinct J-waves. 3. The T-wave inversion is usually in leads V3-V6 (in contrast to Wellens\\' syndrome, in which they are V2-V4) 4. The T-wave inversion does not evolve and is generally stable over time (in contrast to Wellens\\', which evolves ). 5. The leads with T-wave inversion (left precordial) usually have some ST elevation 6. Right precordial leads often have ST elevation typical of classic early repolarization 7. The T-wave inversion in leads V4-V6 is preceded by minimal S-waves 8. The T-wave inversion in leads V4-V6 is preceded by high R-wave amplitude 9. II, III, and aVF also frequently have T-wave inversion. Below are 5 examples, followed by a case of Wellens.\\' I show you the LAD-BER formula calculations, but remember , strictly speaking, this was not studied in ECGs with: 1) T-wave inversion, 2) coved (upwardly convex) ST segments, or 3) LVH. Here is the formula; there is an excel spreadsheet down the right side of this blog: (1.196 x STE at 60 ms after the J-point in V3 in mm) + (0.059 x computerized QTc) - (0.326 x R-wave Amplitude in V4 in mm).\\xa0 A value greater than 23.4 is quite sensitive and specific for LAD occlusion.\\nBenign T-wave inversion: There is ST elevation in V2 and V3, with T-wave inversion in left precordial leads.\\xa0 QTc = 395ms, formula value (to determine if the STE is STEMI or not) = 21.13 (< 23.4 is early repol).\\xa0 Note the prominent J-waves, the minimal S-waves and the prominent R amplitude in leads with T-wave inversion.\\nBenign T-wave inversion and probable LVH.\\xa0 Scary ST elevation, right?\\xa0 But QTc is 421ms and formula value is 20.17.\\xa0\\nBenign T-wave Inversion?\\xa0 QTc 398, formula 17.6.\\xa0 This one was not stable.\\xa0 The next day it was gone.\\xa0 It was recorded in a young black male with chest pain.\\xa0 There were negative serial troponins but no angiogram.\\xa0 Dr. Wang considered it to be BTWI.\\nWellens\\' syndrome.\\xa0\\xa0 Note the evolution from A to C.\\xa0 It begins with terminal T-wave inversion (biphasic) in lead V2, later extends to V3 and V4, and still later becomes deep and symmetric and only then extends to V6.\\xa0 There was a critical LAD stenosis.\\xa0 In this case I do not know what the computerized QTc was.\\xa0 This is taken from an article I wrote in EM Clinics of North America 2006;24(1):53-89.\\nCases come from all over the world. Patient identifiers have been redacted or patient consent has been obtained. The contents of this site have not been reviewed nor approved by Hennepin County Medical Center and any views or opinions expressed herein do not necessarily reflect the views or opinions of Hennepin County Medical Center.\\nAs of March 10, 2018, I\\'ve decided to run ads here. All ad revenue will go to my ECG research projects. We need funding. Up',\n",
       " 'input_ids': [0,\n",
       "  17521,\n",
       "  4932,\n",
       "  255,\n",
       "  12,\n",
       "  20654,\n",
       "  96,\n",
       "  21747,\n",
       "  31,\n",
       "  21956,\n",
       "  21243,\n",
       "  5404,\n",
       "  15,\n",
       "  468,\n",
       "  40247,\n",
       "  479,\n",
       "  345,\n",
       "  32,\n",
       "  171,\n",
       "  4400,\n",
       "  118,\n",
       "  20643,\n",
       "  9,\n",
       "  255,\n",
       "  12,\n",
       "  20654,\n",
       "  11,\n",
       "  21747,\n",
       "  4,\n",
       "  50141,\n",
       "  166,\n",
       "  32,\n",
       "  144,\n",
       "  3915,\n",
       "  59,\n",
       "  16,\n",
       "  2871,\n",
       "  15796,\n",
       "  255,\n",
       "  12,\n",
       "  20654,\n",
       "  11,\n",
       "  21747,\n",
       "  4,\n",
       "  50141,\n",
       "  2647,\n",
       "  1290,\n",
       "  108,\n",
       "  14115,\n",
       "  16,\n",
       "  1605,\n",
       "  2702,\n",
       "  6,\n",
       "  25,\n",
       "  24,\n",
       "  39427,\n",
       "  41,\n",
       "  21541,\n",
       "  2008,\n",
       "  226,\n",
       "  2606,\n",
       "  41281,\n",
       "  13310,\n",
       "  4,\n",
       "  50141,\n",
       "  38,\n",
       "  33,\n",
       "  484,\n",
       "  4570,\n",
       "  15,\n",
       "  42,\n",
       "  131,\n",
       "  259,\n",
       "  16,\n",
       "  65,\n",
       "  14,\n",
       "  924,\n",
       "  5,\n",
       "  1445,\n",
       "  10795,\n",
       "  479,\n",
       "  2044,\n",
       "  4400,\n",
       "  17129,\n",
       "  16,\n",
       "  22,\n",
       "  17521,\n",
       "  4932,\n",
       "  255,\n",
       "  12,\n",
       "  20654,\n",
       "  96,\n",
       "  21747,\n",
       "  1297,\n",
       "  61,\n",
       "  34,\n",
       "  251,\n",
       "  57,\n",
       "  4984,\n",
       "  4,\n",
       "  38,\n",
       "  78,\n",
       "  794,\n",
       "  24,\n",
       "  1602,\n",
       "  11,\n",
       "  31530,\n",
       "  18,\n",
       "  31046,\n",
       "  4,\n",
       "  50141,\n",
       "  85,\n",
       "  16,\n",
       "  10,\n",
       "  2340,\n",
       "  17390,\n",
       "  3059,\n",
       "  19,\n",
       "  419,\n",
       "  2851,\n",
       "  19231,\n",
       "  1938,\n",
       "  4,\n",
       "  50141,\n",
       "  229,\n",
       "  4,\n",
       "  9705,\n",
       "  682,\n",
       "  8069,\n",
       "  24,\n",
       "  4,\n",
       "  50143,\n",
       "  91,\n",
       "  7123,\n",
       "  11270,\n",
       "  28393,\n",
       "  31,\n",
       "  70,\n",
       "  365,\n",
       "  6,\n",
       "  35784,\n",
       "  1484,\n",
       "  54,\n",
       "  56,\n",
       "  23,\n",
       "  513,\n",
       "  65,\n",
       "  2673,\n",
       "  148,\n",
       "  3010,\n",
       "  23,\n",
       "  289,\n",
       "  4734,\n",
       "  2462,\n",
       "  179,\n",
       "  413,\n",
       "  3067,\n",
       "  824,\n",
       "  36,\n",
       "  8569,\n",
       "  38,\n",
       "  173,\n",
       "  43,\n",
       "  8,\n",
       "  278,\n",
       "  4364,\n",
       "  5,\n",
       "  6560,\n",
       "  1200,\n",
       "  9,\n",
       "  27197,\n",
       "  255,\n",
       "  12,\n",
       "  20654,\n",
       "  11,\n",
       "  21747,\n",
       "  4,\n",
       "  50141,\n",
       "  8783,\n",
       "  58,\n",
       "  909,\n",
       "  4,\n",
       "  50141,\n",
       "  155,\n",
       "  4,\n",
       "  406,\n",
       "  207,\n",
       "  9,\n",
       "  909,\n",
       "  604,\n",
       "  8,\n",
       "  50141,\n",
       "  112,\n",
       "  207,\n",
       "  9,\n",
       "  909,\n",
       "  390,\n",
       "  56,\n",
       "  42,\n",
       "  2609,\n",
       "  4,\n",
       "  50141,\n",
       "  112,\n",
       "  9,\n",
       "  654,\n",
       "  2831,\n",
       "  1104,\n",
       "  1484,\n",
       "  56,\n",
       "  24,\n",
       "  4,\n",
       "  50141,\n",
       "  14747,\n",
       "  31,\n",
       "  41,\n",
       "  290,\n",
       "  4,\n",
       "  398,\n",
       "  207,\n",
       "  24971,\n",
       "  36,\n",
       "  466,\n",
       "  9,\n",
       "  14512,\n",
       "  43,\n",
       "  909,\n",
       "  14705,\n",
       "  5180,\n",
       "  601,\n",
       "  12,\n",
       "  1646,\n",
       "  6,\n",
       "  24,\n",
       "  21,\n",
       "  21853,\n",
       "  7664,\n",
       "  30,\n",
       "  1046,\n",
       "  333,\n",
       "  4,\n",
       "  38,\n",
       "  33,\n",
       "  7123,\n",
       "  209,\n",
       "  6560,\n",
       "  11270,\n",
       "  28393,\n",
       "  6,\n",
       "  8,\n",
       "  99,\n",
       "  5315,\n",
       "  162,\n",
       "  16,\n",
       "  35,\n",
       "  112,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "num = random.randint(0,len(preprocessed_dataset))\n",
    "preprocessed_dataset[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72ff077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets just use 100k rows for this task and split(80/10/10)\n",
    "split = preprocessed_dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "train_val = split[\"train\"].train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "train_dataset = train_val[\"train\"]\n",
    "test_dataset = split[\"test\"]\n",
    "val_dataset = train_val[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcaa2bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "actual_data = DatasetDict(\n",
    "    {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd263de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c62fa6bc5934e79af5df90d335ffdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a53c82e023456ba79a25ab6606bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/74 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047c6c1052ce415484d71ebe6bf073be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4b579b7d224a0b8078adb6ad7cffa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceaa626d9ff47838bf4466364304b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  28%|##7       | 66.6MB /  240MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c78636273314315b6988b809549894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/74 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71a014aac6c48c2b0ea3af1daf5afe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dadc23c74354d4cb31f4890fd30c100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ca7d837b494369a7db3b42640ccb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  28%|##7       | 66.7MB /  241MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654bd834ebc6418c9ed0300b98c3bd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/74 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a94db0d9684d0491a8b2d4fd0363f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35a2cb3e1d447ce8eadfd57689192e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9a405afe674ba996b09fc87c70f6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  28%|##7       | 66.7MB /  240MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332b8dde9a094629a7bcd0da78024de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0209c97c209487ba85acf2f93f4e9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2740b7b518ec469da363c3dbe1a66735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4eeb4a245a4e3186a4392c03c5eadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6da9262bd142a283d62ad3fafecb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        : 100%|#########9| 79.8MB / 80.2MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017829be57394bfc96e0841a01f5a2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c4a0f8e8dd4b81968b27dfd8d82bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c76203cf5847f49a982ce5f6f5ea9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bd553cc3324566af0ca42c0e5443f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841a65ace3f440019a4215815de1937e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "                                        :  33%|###3      | 66.7MB /  199MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/optimization-hashira/ai-text-detection-dataset/commit/953ecda8458d2b2ebc4f5e752f40f311ecffdeee', commit_message='Upload dataset', commit_description='', oid='953ecda8458d2b2ebc4f5e752f40f311ecffdeee', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/optimization-hashira/ai-text-detection-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='optimization-hashira/ai-text-detection-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_data.push_to_hub(repo_id=\"optimization-hashira/ai-text-detection-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5037c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d47d39174348dc8257100ab0768afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/305044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_dataset.save_to_disk(dataset_path=\"./dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced43147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7199977708133909, 0.20000065564312033, 0.0800015735434888)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) / len(preprocessed_dataset), len(test_dataset) / len(\n",
    "    preprocessed_dataset\n",
    "), len(val_dataset) / len(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58de6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792d2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_path = \"optimization-hashira/roberta-ai-text-detector\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5629af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"optimization-hashira/ai-text-detection-dataset\"\n",
    "\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "val_dataset = dataset[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc271dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110500/3964794248.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"ai_text_detector\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cdaca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a5415655854fafa47aa4384865b853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c750dc2a5a44f3d921fc0e2eae6ab23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8857c4819a241289a54a61a13106a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lit/train/data-00002-of-00003.arrow:   2%|2         | 11.0MB /  468MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8188195e96648dc847093607c375db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lit/train/data-00001-of-00003.arrow:   0%|          | 1.82MB /  469MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f2b6075895452fbcc31669c685aec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...split/val/data-00000-of-00001.arrow:   1%|          |  867kB /  157MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ae5adb3a204d4b9f8402f232616bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...lit/train/data-00000-of-00003.arrow:   0%|          |  969kB /  469MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2a0d87dab04a3b8c365831b5c50f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...plit/test/data-00000-of-00001.arrow:  15%|#4        | 58.3MB /  390MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/optimization-hashira/ai-text-detection-dataset/commit/d921c228355189b5a5762afd773ae81adaff8248', commit_message='first commit', commit_description='', oid='d921c228355189b5a5762afd773ae81adaff8248', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/optimization-hashira/ai-text-detection-dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='optimization-hashira/ai-text-detection-dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "# api.create_repo(repo_id=\"optimization-hashira/roberta-ai-text-detector\", private=True)\n",
    "# api.upload_folder(\n",
    "#     folder_path=model_path, repo_id=\"optimization-hashira/roberta-ai-text-detector\",commit_message=\"first commit\"\n",
    "# )\n",
    "\n",
    "api.create_repo(repo_id=\"optimization-hashira/ai-text-detection-dataset\",repo_type=\"dataset\",private=True)\n",
    "api.upload_folder(repo_id=\"optimization-hashira/ai-text-detection-dataset\",\n",
    "                  folder_path=dataset_path,\n",
    "                  repo_type=\"dataset\",\n",
    "                  commit_message=\"first commit\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67510aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd97a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"eval_loss\": 0.07999948412179947,\n",
    "    \"eval_model_preparation_time\": 0.002,\n",
    "    \"eval_accuracy\": 0.9896080906095822,\n",
    "    \"eval_f1\": 0.989469488090888,\n",
    "    \"eval_precision\": 0.9990943548116593,\n",
    "    \"eval_recall\": 0.980028295989208,\n",
    "    \"eval_runtime\": 399.3875,\n",
    "    \"eval_samples_per_second\": 152.756,\n",
    "    \"eval_steps_per_second\": 9.55,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
