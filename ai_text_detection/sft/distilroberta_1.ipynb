{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7914522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raid.utils import load_data\n",
    "\n",
    "train_df = load_data(split=\"train\",include_adversarial=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f56d0f",
   "metadata": {},
   "source": [
    "Human model will have NaN decoding duh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "496aefc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'greedy', 'sampling'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['decoding'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9d0f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "books        62335\n",
       "news         62300\n",
       "reddit       62265\n",
       "wiki         62265\n",
       "recipes      62020\n",
       "poetry       61985\n",
       "abstracts    61810\n",
       "reviews      33005\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['domain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fb602f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "books        1781\n",
       "news         1780\n",
       "reddit       1779\n",
       "wiki         1779\n",
       "recipes      1772\n",
       "poetry       1771\n",
       "abstracts    1766\n",
       "reviews       943\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"model\"] == \"human\"][\n",
    "    \"domain\"\n",
    "].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f8475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_df['model'] = np.where(train_df['model'] == 'human',0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d189e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "1    454614\n",
       "0     13371\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['model'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb687be",
   "metadata": {},
   "source": [
    "The class imbalance for Ai vs Human is 35:1. So to combat this we have to use class weighting. Adding this weight = (total number of samples)/(number of classes * number of samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107f4fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>e5e058ce-be2b-459d-af36-32532aaba5ff</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>f95b107b-d176-4af5-90f7-4d0bb20caf93</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>EdgeFlow: Achieving Practical Interactive Segm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>856d8972-9e3d-4544-babc-0fe16f21e04d</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Semi-supervised Contrastive Learning for Label...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>72c41b8d-0069-4886-b734-a4000ffca286</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Attention-Based 3D Seismic Fault Segmentation ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff  e5e058ce-be2b-459d-af36-32532aaba5ff   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93  f95b107b-d176-4af5-90f7-4d0bb20caf93   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d  856d8972-9e3d-4544-babc-0fe16f21e04d   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286  72c41b8d-0069-4886-b734-a4000ffca286   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  e5e058ce-be2b-459d-af36-32532aaba5ff      0      NaN                NaN   \n",
       "1  f95b107b-d176-4af5-90f7-4d0bb20caf93      0      NaN                NaN   \n",
       "2  856d8972-9e3d-4544-babc-0fe16f21e04d      0      NaN                NaN   \n",
       "3  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524      0      NaN                NaN   \n",
       "4  72c41b8d-0069-4886-b734-a4000ffca286      0      NaN                NaN   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  FUTURE-AI: Guiding Principles and Consensus Re...    NaN   \n",
       "1   none  abstracts  EdgeFlow: Achieving Practical Interactive Segm...    NaN   \n",
       "2   none  abstracts  Semi-supervised Contrastive Learning for Label...    NaN   \n",
       "3   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "4   none  abstracts  Attention-Based 3D Seismic Fault Segmentation ...    NaN   \n",
       "\n",
       "                                          generation  \n",
       "0  The recent advancements in artificial intellig...  \n",
       "1  High-quality training data play a key role in ...  \n",
       "2  The success of deep learning methods in medica...  \n",
       "3  Simultaneous segmentation of multiple organs f...  \n",
       "4  Detection faults in seismic data is a crucial ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ee0343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>High-quality training data play a key role in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The success of deep learning methods in medica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Detection faults in seismic data is a crucial ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            content\n",
       "0      0  The recent advancements in artificial intellig...\n",
       "1      0  High-quality training data play a key role in ...\n",
       "2      0  The success of deep learning methods in medica...\n",
       "3      0  Simultaneous segmentation of multiple organs f...\n",
       "4      0  Detection faults in seismic data is a crucial ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(\n",
    "    [\n",
    "        \"id\",\n",
    "        \"decoding\",\n",
    "        \"domain\",\n",
    "        \"adv_source_id\",\n",
    "        \"source_id\",\n",
    "        \"repetition_penalty\",\n",
    "        \"attack\",\n",
    "        \"title\",\n",
    "        \"prompt\"\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "train_df = train_df.rename(columns={\n",
    "    \"generation\" : \"content\",\n",
    "    \"model\" : \"label\"\n",
    "})\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a35f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17.5         0.51470588]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "labels = train_df[\"label\"]\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\",classes=np.unique(labels),y=labels)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9336348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "model_path = \"distilbert/distilroberta-base\"\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=2)\n",
    "tokeinzer = AutoTokenizer.from_pretrained(model_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d96935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df[\"label\"])\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "\n",
    "val_data,test_data = train_test_split(val_data, test_size=0.5, random_state=42, stratify=val_data[\"label\"])\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "180fbe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'content'],\n",
       "    num_rows: 421186\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46cd38f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n",
    "val_dataset = val_dataset.remove_columns([\"__index_level_0__\"])\n",
    "test_dataset = test_dataset.remove_columns([\"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee500c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb7aa4dc4a14d66a3becd3dd4af43a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/421186 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80cb13642f094a98b523b329127b28ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb13fee4ffce40dfa745652ddc7f4292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_fn(batch):\n",
    "    return tokeinzer(batch[\"content\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_fn, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_fn, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957de756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14465/165361310.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=3e-5,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"distilroberta-noadv-raid\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokeinzer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db248083",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CustomTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/transformers/trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/transformers/trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2670\u001b[0m )\n\u001b[1;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2678\u001b[0m ):\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/transformers/trainer.py:4009\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4009\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4011\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4014\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4015\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: CustomTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
