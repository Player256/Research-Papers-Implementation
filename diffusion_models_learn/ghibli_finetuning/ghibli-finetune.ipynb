{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/shubham1921/real-to-ghibli-image-dataset-5k-paired-images?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543M/543M [00:05<00:00, 104MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/shubham1921/real-to-ghibli-image-dataset-5k-paired-images/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"shubham1921/real-to-ghibli-image-dataset-5k-paired-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n",
    "\n",
    "MODEL_ID = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "llm = LLM(\n",
    "    model=MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\",\n",
    "    max_model_len=4096,\n",
    "    limit_mm_per_prompt={\"image\": 1},\n",
    ")\n",
    "\n",
    "input_image_dir = \"./dataset/images\"\n",
    "output_description_dir = \"ghibli_style_descriptions\"\n",
    "os.makedirs(output_description_dir, exist_ok=True)\n",
    "\n",
    "image_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")\n",
    "files = [\n",
    "    os.path.join(input_image_dir, f)\n",
    "    for f in os.listdir(input_image_dir)\n",
    "    if f.lower().endswith(image_extensions)\n",
    "]\n",
    "\n",
    "if not files:\n",
    "    print(f\"No images found in {input_image_dir}.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Found {len(files)} images.\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "batches = np.array_split(files, max(1, len(files) // BATCH_SIZE))\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=128)\n",
    "\n",
    "for batch_files in tqdm(batches, desc=\"Processing batches\"):\n",
    "    requests = []\n",
    "\n",
    "    for image_path in batch_files:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        prompt = \"User: <image>\\nPlease provide a detailed description of the image including its style and the way it is drawn. Its drawing style is ghibli style. So give description in accordance with that.\\nAssistant:\"\n",
    "\n",
    "        requests.append({\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}})\n",
    "\n",
    "    outputs = llm.generate(requests, sampling_params)\n",
    "\n",
    "    for i, output in enumerate(outputs):\n",
    "        image_path = batch_files[i]\n",
    "        description = output.outputs[0].text.strip()\n",
    "\n",
    "        description_with_token = \"<ghibli-style> \" + description\n",
    "\n",
    "        base_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        output_txt_path = os.path.join(output_description_dir, base_filename + \".txt\")\n",
    "\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(description_with_token)\n",
    "            \n",
    "        output_image_path = os.path.join(\n",
    "            output_description_dir, os.path.basename(image_path)\n",
    "        )\n",
    "        shutil.copy(image_path, output_image_path)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/huggingface/diffusers/main/examples/text_to_image/train_text_to_image_lora.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created metadata.jsonl with 2500 entries at ./ghibli_style_descriptions/metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dataset_folder = (\n",
    "    \"./ghibli_style_descriptions\"  \n",
    ")\n",
    "metadata_file = os.path.join(dataset_folder, \"metadata.jsonl\")\n",
    "style_token = (\n",
    "    \"<ghibli-style>\"  \n",
    ")\n",
    "\n",
    "data_entries = []\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "        image_name = filename\n",
    "        text_name = os.path.splitext(filename)[0] + \".txt\"\n",
    "        text_path = os.path.join(dataset_folder, text_name)\n",
    "        if os.path.exists(text_path):\n",
    "            try:\n",
    "                with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    caption = f.read().strip()\n",
    "\n",
    "                entry = {\"file_name\": image_name, \"text\": caption}\n",
    "                data_entries.append(entry)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text file {text_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: No matching text file found for {image_name}. Skipping.\")\n",
    "\n",
    "with open(metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in data_entries:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Created metadata.jsonl with {len(data_entries)} entries at {metadata_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 19:08:30,632] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Multiple distributions found for package optimum. Picked distribution: optimum-quanto\n",
      "05/07/2025 19:08:33 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: bf16\n",
      "\n",
      "{'prediction_type', 'variance_type', 'clip_sample_range', 'dynamic_thresholding_ratio', 'timestep_spacing', 'rescale_betas_zero_snr', 'thresholding', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
      "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor', 'force_upcast', 'scaling_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dropout', 'only_cross_attention', 'conv_in_kernel', 'time_embedding_dim', 'addition_embed_type', 'mid_block_type', 'addition_embed_type_num_heads', 'conv_out_kernel', 'resnet_time_scale_shift', 'time_embedding_act_fn', 'cross_attention_norm', 'class_embeddings_concat', 'upcast_attention', 'reverse_transformer_layers_per_block', 'num_attention_heads', 'attention_type', 'class_embed_type', 'num_class_embeds', 'timestep_post_act', 'encoder_hid_dim', 'addition_time_embed_dim', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'use_linear_projection', 'transformer_layers_per_block', 'resnet_skip_time_act', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'time_embedding_type', 'dual_cross_attention', 'projection_class_embeddings_input_dim'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at runwayml/stable-diffusion-v1-5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Resolving data files: 100%|█████████████| 5001/5001 [00:00<00:00, 437130.65it/s]\n",
      "Downloading data: 100%|██████████████| 2501/2501 [00:00<00:00, 104420.25files/s]\n",
      "Downloading data: 100%|██████████████| 2500/2500 [00:00<00:00, 112969.97files/s]\n",
      "Generating train split: 2500 examples [00:00, 27202.11 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdattucodes\u001b[0m (\u001b[33mdeath-star\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ubuntu/oracle/Research-Papers-Implementation/diffusion_models_learn/basics/wandb/run-20250507_190838-6vxzm7c8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfluent-bush-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/death-star/text2image-fine-tune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/death-star/text2image-fine-tune/runs/6vxzm7c8\u001b[0m\n",
      "05/07/2025 19:08:39 - INFO - __main__ - ***** Running training *****\n",
      "05/07/2025 19:08:39 - INFO - __main__ -   Num examples = 2500\n",
      "05/07/2025 19:08:39 - INFO - __main__ -   Num Epochs = 4\n",
      "05/07/2025 19:08:39 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "05/07/2025 19:08:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "05/07/2025 19:08:39 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/07/2025 19:08:39 - INFO - __main__ -   Total optimization steps = 2500\n",
      "Steps:  20%|▌  | 500/2500 [05:12<21:31,  1.55it/s, lr=9.98e-5, step_loss=0.0596]05/07/2025 19:13:52 - INFO - accelerate.accelerator - Saving current state to lora-ghibli-finetuned/checkpoint-500\n",
      "05/07/2025 19:13:54 - INFO - accelerate.checkpointing - Model weights saved in lora-ghibli-finetuned/checkpoint-500/model.safetensors\n",
      "05/07/2025 19:13:54 - INFO - accelerate.checkpointing - Optimizer state saved in lora-ghibli-finetuned/checkpoint-500/optimizer.bin\n",
      "05/07/2025 19:13:54 - INFO - accelerate.checkpointing - Scheduler state saved in lora-ghibli-finetuned/checkpoint-500/scheduler.bin\n",
      "05/07/2025 19:13:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in lora-ghibli-finetuned/checkpoint-500/sampler.bin\n",
      "05/07/2025 19:13:54 - INFO - accelerate.checkpointing - Random states saved in lora-ghibli-finetuned/checkpoint-500/random_states_0.pkl\n",
      "Model weights saved in lora-ghibli-finetuned/checkpoint-500/pytorch_lora_weights.safetensors\n",
      "05/07/2025 19:13:54 - INFO - __main__ - Saved state to lora-ghibli-finetuned/checkpoint-500\n",
      "Steps:  25%|█▎   | 625/2500 [06:36<19:05,  1.64it/s, lr=9.9e-5, step_loss=0.136]\n",
      "model_index.json: 100%|████████████████████████| 541/541 [00:00<00:00, 3.93MB/s]\u001b[A\n",
      "\n",
      "Fetching 13 files:   0%|                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "preprocessor_config.json: 100%|████████████████| 342/342 [00:00<00:00, 2.47MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "config.json: 100%|█████████████████████████| 4.72k/4.72k [00:00<00:00, 27.9MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Fetching 13 files:  23%|█████▊                   | 3/13 [00:00<00:00, 24.75it/s]\u001b[A\n",
      "\n",
      "model.safetensors:   2%|▎                   | 21.0M/1.22G [00:00<00:07, 170MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|▊                   | 52.4M/1.22G [00:00<00:04, 238MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|█▉                   | 115M/1.22G [00:00<00:02, 380MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|██▉                  | 168M/1.22G [00:00<00:02, 427MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|███▊                 | 220M/1.22G [00:00<00:02, 451MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▋                | 273M/1.22G [00:00<00:02, 470MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  27%|█████▌               | 325M/1.22G [00:00<00:01, 477MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  31%|██████▌              | 377M/1.22G [00:00<00:01, 469MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  35%|███████▍             | 430M/1.22G [00:00<00:01, 475MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  40%|████████▎            | 482M/1.22G [00:01<00:01, 472MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  44%|█████████▏           | 535M/1.22G [00:01<00:01, 477MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|██████████▏          | 587M/1.22G [00:01<00:01, 477MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  53%|███████████          | 640M/1.22G [00:01<00:01, 486MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  57%|███████████▉         | 692M/1.22G [00:01<00:01, 480MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  61%|████████████▊        | 744M/1.22G [00:01<00:00, 485MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  66%|█████████████▊       | 797M/1.22G [00:01<00:00, 479MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  70%|██████████████▋      | 849M/1.22G [00:01<00:00, 484MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  75%|███████████████▊     | 912M/1.22G [00:01<00:00, 498MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  79%|████████████████▋    | 965M/1.22G [00:02<00:00, 497MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  84%|████████████████▋   | 1.02G/1.22G [00:02<00:00, 495MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  88%|█████████████████▌  | 1.07G/1.22G [00:02<00:00, 482MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  92%|██████████████████▍ | 1.12G/1.22G [00:02<00:00, 477MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|████████████████████| 1.22G/1.22G [00:02<00:00, 461MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Fetching 13 files: 100%|████████████████████████| 13/13 [00:02<00:00,  4.83it/s]\u001b[A\n",
      "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  57%|███████▍     | 4/7 [00:00<00:00, 31.70it/s]\u001b[ALoaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Instantiating AutoencoderKL model under default dtype torch.bfloat16.\n",
      "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor', 'force_upcast', 'scaling_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /home/ubuntu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00,  8.59it/s]\n",
      "05/07/2025 19:15:19 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a landscape with trees and hills, <ghibli-style>.\n",
      "Steps:  40%|▊ | 1000/2500 [10:44<15:55,  1.57it/s, lr=8.54e-5, step_loss=0.0805]05/07/2025 19:19:24 - INFO - accelerate.accelerator - Saving current state to lora-ghibli-finetuned/checkpoint-1000\n",
      "05/07/2025 19:19:26 - INFO - accelerate.checkpointing - Model weights saved in lora-ghibli-finetuned/checkpoint-1000/model.safetensors\n",
      "05/07/2025 19:19:26 - INFO - accelerate.checkpointing - Optimizer state saved in lora-ghibli-finetuned/checkpoint-1000/optimizer.bin\n",
      "05/07/2025 19:19:26 - INFO - accelerate.checkpointing - Scheduler state saved in lora-ghibli-finetuned/checkpoint-1000/scheduler.bin\n",
      "05/07/2025 19:19:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in lora-ghibli-finetuned/checkpoint-1000/sampler.bin\n",
      "05/07/2025 19:19:26 - INFO - accelerate.checkpointing - Random states saved in lora-ghibli-finetuned/checkpoint-1000/random_states_0.pkl\n",
      "Model weights saved in lora-ghibli-finetuned/checkpoint-1000/pytorch_lora_weights.safetensors\n",
      "05/07/2025 19:19:26 - INFO - __main__ - Saved state to lora-ghibli-finetuned/checkpoint-1000\n",
      "Steps:  50%|█▌ | 1250/2500 [13:24<12:37,  1.65it/s, lr=6.91e-5, step_loss=0.108]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:00<00:00, 12.86it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.bfloat16.\n",
      "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor', 'force_upcast', 'scaling_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /home/ubuntu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 11.34it/s]\u001b[A\n",
      "05/07/2025 19:22:07 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a landscape with trees and hills, <ghibli-style>.\n",
      "Steps:  60%|█▊ | 1500/2500 [16:15<10:08,  1.64it/s, lr=5.01e-5, step_loss=0.253]05/07/2025 19:24:55 - INFO - accelerate.accelerator - Saving current state to lora-ghibli-finetuned/checkpoint-1500\n",
      "05/07/2025 19:24:57 - INFO - accelerate.checkpointing - Model weights saved in lora-ghibli-finetuned/checkpoint-1500/model.safetensors\n",
      "05/07/2025 19:24:57 - INFO - accelerate.checkpointing - Optimizer state saved in lora-ghibli-finetuned/checkpoint-1500/optimizer.bin\n",
      "05/07/2025 19:24:57 - INFO - accelerate.checkpointing - Scheduler state saved in lora-ghibli-finetuned/checkpoint-1500/scheduler.bin\n",
      "05/07/2025 19:24:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in lora-ghibli-finetuned/checkpoint-1500/sampler.bin\n",
      "05/07/2025 19:24:57 - INFO - accelerate.checkpointing - Random states saved in lora-ghibli-finetuned/checkpoint-1500/random_states_0.pkl\n",
      "Model weights saved in lora-ghibli-finetuned/checkpoint-1500/pytorch_lora_weights.safetensors\n",
      "05/07/2025 19:24:57 - INFO - __main__ - Saved state to lora-ghibli-finetuned/checkpoint-1500\n",
      "Steps:  75%|█▌| 1875/2500 [20:11<06:16,  1.66it/s, lr=2.22e-5, step_loss=0.0623]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:00<00:00, 12.72it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.bfloat16.\n",
      "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor', 'force_upcast', 'scaling_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /home/ubuntu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 11.32it/s]\u001b[A\n",
      "05/07/2025 19:28:51 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a landscape with trees and hills, <ghibli-style>.\n",
      "Steps:  80%|██▍| 2000/2500 [21:41<05:13,  1.59it/s, lr=1.47e-5, step_loss=0.601]05/07/2025 19:30:21 - INFO - accelerate.accelerator - Saving current state to lora-ghibli-finetuned/checkpoint-2000\n",
      "05/07/2025 19:30:23 - INFO - accelerate.checkpointing - Model weights saved in lora-ghibli-finetuned/checkpoint-2000/model.safetensors\n",
      "05/07/2025 19:30:23 - INFO - accelerate.checkpointing - Optimizer state saved in lora-ghibli-finetuned/checkpoint-2000/optimizer.bin\n",
      "05/07/2025 19:30:23 - INFO - accelerate.checkpointing - Scheduler state saved in lora-ghibli-finetuned/checkpoint-2000/scheduler.bin\n",
      "05/07/2025 19:30:23 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in lora-ghibli-finetuned/checkpoint-2000/sampler.bin\n",
      "05/07/2025 19:30:23 - INFO - accelerate.checkpointing - Random states saved in lora-ghibli-finetuned/checkpoint-2000/random_states_0.pkl\n",
      "Model weights saved in lora-ghibli-finetuned/checkpoint-2000/pytorch_lora_weights.safetensors\n",
      "05/07/2025 19:30:23 - INFO - __main__ - Saved state to lora-ghibli-finetuned/checkpoint-2000\n",
      "Steps: 100%|██| 2500/2500 [26:58<00:00,  1.66it/s, lr=6.17e-11, step_loss=0.159]05/07/2025 19:35:38 - INFO - accelerate.accelerator - Saving current state to lora-ghibli-finetuned/checkpoint-2500\n",
      "05/07/2025 19:35:40 - INFO - accelerate.checkpointing - Model weights saved in lora-ghibli-finetuned/checkpoint-2500/model.safetensors\n",
      "05/07/2025 19:35:40 - INFO - accelerate.checkpointing - Optimizer state saved in lora-ghibli-finetuned/checkpoint-2500/optimizer.bin\n",
      "05/07/2025 19:35:40 - INFO - accelerate.checkpointing - Scheduler state saved in lora-ghibli-finetuned/checkpoint-2500/scheduler.bin\n",
      "05/07/2025 19:35:40 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in lora-ghibli-finetuned/checkpoint-2500/sampler.bin\n",
      "05/07/2025 19:35:40 - INFO - accelerate.checkpointing - Random states saved in lora-ghibli-finetuned/checkpoint-2500/random_states_0.pkl\n",
      "Model weights saved in lora-ghibli-finetuned/checkpoint-2500/pytorch_lora_weights.safetensors\n",
      "05/07/2025 19:35:40 - INFO - __main__ - Saved state to lora-ghibli-finetuned/checkpoint-2500\n",
      "Steps: 100%|████████| 2500/2500 [27:00<00:00,  1.66it/s, lr=0, step_loss=0.0619]{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:00<00:00, 12.77it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.bfloat16.\n",
      "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor', 'force_upcast', 'scaling_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /home/ubuntu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 11.38it/s]\u001b[A\n",
      "05/07/2025 19:35:41 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a landscape with trees and hills, <ghibli-style>.\n",
      "Model weights saved in lora-ghibli-finetuned/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.bfloat16.\n",
      "{'dropout', 'only_cross_attention', 'conv_in_kernel', 'time_embedding_dim', 'addition_embed_type', 'mid_block_type', 'addition_embed_type_num_heads', 'conv_out_kernel', 'resnet_time_scale_shift', 'time_embedding_act_fn', 'cross_attention_norm', 'class_embeddings_concat', 'upcast_attention', 'reverse_transformer_layers_per_block', 'num_attention_heads', 'attention_type', 'class_embed_type', 'num_class_embeds', 'timestep_post_act', 'encoder_hid_dim', 'addition_time_embed_dim', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'use_linear_projection', 'transformer_layers_per_block', 'resnet_skip_time_act', 'encoder_hid_dim_type', 'time_cond_proj_dim', 'time_embedding_type', 'dual_cross_attention', 'projection_class_embeddings_input_dim'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /home/ubuntu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  14%|█▊           | 1/7 [00:00<00:05,  1.06it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:01<00:00,  3.89it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.bfloat16.\n",
      "{'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor', 'force_upcast', 'scaling_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /home/ubuntu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  86%|███████████▏ | 6/7 [00:01<00:00,  4.36it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:01<00:00,  3.81it/s]\u001b[A\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "05/07/2025 19:35:53 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a landscape with trees and hills, <ghibli-style>.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▃▂▂▅▄▄▁▃▆▆▃▃▄▅▅▄▅▅█▂▇▃▁▄▅▆▃▁▁▂▅▅▂▃▄▃▁▂▅▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.06193\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfluent-bush-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/death-star/text2image-fine-tune/runs/6vxzm7c8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/death-star/text2image-fine-tune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250507_190838-6vxzm7c8/logs\u001b[0m\n",
      "/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2310: UserWarning: Run (6vxzm7c8) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
      "Steps: 100%|████████| 2500/2500 [27:28<00:00,  1.52it/s, lr=0, step_loss=0.0619]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n",
    "  --train_data_dir=\"/home/ubuntu/oracle/Research-Papers-Implementation/diffusion_models_learn/basics/ghibli_style_descriptions\" \\\n",
    "  --caption_column=\"text\" \\\n",
    "  --resolution=512 \\\n",
    "  --center_crop \\\n",
    "  --random_flip \\\n",
    "  --train_batch_size=4 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --max_train_steps=2500 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --max_grad_norm=1.0 \\\n",
    "  --lr_scheduler=\"cosine\" \\\n",
    "  --output_dir=\"lora-ghibli-finetuned\" \\\n",
    "  --seed=42 \\\n",
    "  --mixed_precision=\"bf16\" \\\n",
    "  --report_to=\"wandb\" \\\n",
    "  --checkpointing_steps=500 \\\n",
    "  --rank=64 \\\n",
    "  --validation_prompt=\"a landscape with trees and hills, <ghibli-style>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 19:47:41,468] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/idm/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/envs/idm/compiler_compat/ld: warning: librt.so.1, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/opt/conda/envs/idm/compiler_compat/ld: warning: libpthread.so.0, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/opt/conda/envs/idm/compiler_compat/ld: warning: libm.so.6, needed by /usr/local/cuda/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/opt/conda/envs/idm/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/opt/conda/envs/idm/compiler_compat/ld: /usr/local/lib/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/opt/conda/envs/idm/compiler_compat/ld: /usr/local/lib/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3134d90c924cb6aea15994c41545cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aad4d1d84ba41cc81e8f1ecd5b94333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "lora_weights_path = \"./lora-ghibli-finetuned\"\n",
    "\n",
    "input_image = \"test.jpg\"\n",
    "\n",
    "output_image_path = \"ghibli-test.jpg\"\n",
    "style_token = \"<ghibli-style>\"\n",
    "\n",
    "prompt = \"A women with a horse and other animals\"\n",
    "\n",
    "conversion_strength = 0.8\n",
    "inference_steps = 50\n",
    "guidance_scale = 7.5\n",
    "model_input_resolution = 512\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.load_lora_weights(lora_weights_path)\n",
    "\n",
    "image = Image.open(input_image).convert(\"RGB\")\n",
    "image = image.resize((model_input_resolution, model_input_resolution))\n",
    "\n",
    "prompt = f\"{prompt}, {style_token}\"\n",
    "\n",
    "output_image = pipe(\n",
    "    prompt=prompt,\n",
    "    image=image,\n",
    "    strength=conversion_strength,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=inference_steps,\n",
    ").images[0]\n",
    "\n",
    "output_image.save(output_image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
