{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47e6f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...      0\n",
       "1  The China relay network has released a signifi...      1\n",
       "2  China\\nThe goal of this project involves achie...      1\n",
       "3  The project aims to achieve an accuracy level ...      0\n",
       "4  Scientists can learn about how galaxies form a...      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_dir = './data/train'\n",
    "\n",
    "ref_df = pd.read_csv('./data/train.csv')\n",
    "train_df = pd.DataFrame(columns=['text','labels'])\n",
    "\n",
    "for _, row in ref_df.iterrows():\n",
    "    id = row['id']\n",
    "    real_text_id = row['real_text_id']\n",
    "    \n",
    "    file_prefix = f'article_{id:04d}'\n",
    "    \n",
    "    file_path_dir = os.path.join(train_dir, file_prefix)\n",
    "\n",
    "    file_1_path = os.path.join(file_path_dir, 'file_1.txt')\n",
    "    file_2_path = os.path.join(file_path_dir, \"file_2.txt\")\n",
    "    \n",
    "    with open(file_1_path, 'r', encoding='utf-8') as f:\n",
    "        file_1_text = f.read().strip()\n",
    "    with open(file_2_path, 'r', encoding='utf-8') as f:\n",
    "        file_2_text = f.read().strip()\n",
    "    \n",
    "    if real_text_id == 1:\n",
    "        train_df = pd.concat([train_df, pd.DataFrame({'text': [file_1_text], 'labels': [0]})], ignore_index=True)\n",
    "        train_df = pd.concat([train_df, pd.DataFrame({'text': [file_2_text], 'labels': [1]})], ignore_index=True)\n",
    "    else:\n",
    "        train_df = pd.concat([train_df, pd.DataFrame({'text': [file_1_text], 'labels': [1]})], ignore_index=True)\n",
    "        train_df = pd.concat([train_df, pd.DataFrame({'text': [file_2_text], 'labels': [0]})], ignore_index=True)\n",
    "        \n",
    "        \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f886165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, PreTrainedModel\n",
    "\n",
    "\n",
    "class DesklibAIDetectionModel(PreTrainedModel):\n",
    "    config_class = AutoConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Initialize the base transformer model.\n",
    "        self.model = AutoModel.from_config(config)\n",
    "        # Define a classifier head.\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "        # Initialize weights (handled by PreTrainedModel)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        # Forward pass through the transformer\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        # Mean pooling\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        pooled_output = sum_embeddings / sum_mask\n",
    "\n",
    "        # Classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1), labels.float())\n",
    "\n",
    "        output = {\"logits\": logits}\n",
    "        if loss is not None:\n",
    "            output[\"loss\"] = loss\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c13759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/idm/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = DebertaAIDetectionModel(config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30524c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert/distilroberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"openai-community/roberta-base-openai-detector\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"openai-community/roberta-base-openai-detector\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5f9883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba7a7d9136b4653a797397ef918d09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f359fb370f84c31baf11b8788344aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "data = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = data['train']\n",
    "val_dataset = data['test']\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6a2879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./debert-kaggle\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=None,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  \n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    # preds = (probs >= 0.5).astype(int)\n",
    "    probs = F.softmax(torch.tensor(logits), dim=1).detach().cpu().numpy()\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "\n",
    "    probs_class1 = probs[:, 1]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, probs_class1)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'aucroc' : auc\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d7b3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='129' max='129' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [129/129 00:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Aucroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733000</td>\n",
       "      <td>0.753852</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.659091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.800013</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>1.216903</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.761364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=129, training_loss=0.7982987204725428, metrics={'train_runtime': 59.5071, 'train_samples_per_second': 8.621, 'train_steps_per_second': 2.168, 'total_flos': 134975971399680.0, 'train_loss': 0.7982987204725428, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9389e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id real_text_id\n",
      "0  0            1\n",
      "1  1            2\n",
      "2  2            1\n",
      "3  3            2\n",
      "4  4            2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_df = pd.DataFrame(columns=[\"id\", \"real_text_id\"])\n",
    "test_dir = \"./data/test\"\n",
    "\n",
    "\n",
    "for i in range(len(os.listdir(test_dir))):\n",
    "    file_prefix = f\"article_{i:04d}\"\n",
    "    file_path_dir = os.path.join(test_dir, file_prefix)\n",
    "\n",
    "    file_1_path = os.path.join(file_path_dir, \"file_1.txt\")\n",
    "    file_2_path = os.path.join(file_path_dir, \"file_2.txt\")\n",
    "\n",
    "    with open(file_1_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_1_text = f.read().strip()\n",
    "    with open(file_2_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_2_text = f.read().strip()\n",
    "\n",
    "    inputs_1 = tokenizer(\n",
    "        file_1_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    inputs_2 = tokenizer(\n",
    "        file_2_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    inputs_1.pop(\"token_type_ids\", None)\n",
    "    inputs_2.pop(\"token_type_ids\", None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_1 = model(**inputs_1)\n",
    "        outputs_2 = model(**inputs_2)\n",
    "\n",
    "    logits_1 = outputs_1['logits']\n",
    "    logits_2 = outputs_2['logits']\n",
    "\n",
    "    # probs_1 = torch.sigmoid(logits_1).detach().cpu().numpy()\n",
    "    # probs_2 = torch.sigmoid(logits_2).detach().cpu().numpy()\n",
    "\n",
    "    probs_1 = F.softmax(logits_1, dim=1).detach().cpu().numpy()\n",
    "    probs_2 = F.softmax(logits_2, dim=1).detach().cpu().numpy()\n",
    "\n",
    "    human_prob_file1 = probs_1[0][0]\n",
    "    human_prob_file2 = probs_2[0][0]\n",
    "\n",
    "    real_text_id = 1 if human_prob_file1 > human_prob_file2 else 2\n",
    "\n",
    "    test_df = pd.concat(\n",
    "        [test_df, pd.DataFrame({\"id\": [i], \"real_text_id\": [real_text_id]})],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e3aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"submission_roberta_openai.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
