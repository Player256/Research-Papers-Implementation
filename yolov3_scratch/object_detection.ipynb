{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yolo_v3 From Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image,ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2, is_pred=True):\n",
    "    if is_pred:\n",
    "        # Prediction box\n",
    "        b1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "        b1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "        b1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "        b1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "\n",
    "        # Ground Truth box\n",
    "        b2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "        b2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "        b2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "        b2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "\n",
    "        # Intersection rectangle\n",
    "        x1 = torch.max(b1_x1, b2_x1)\n",
    "        y1 = torch.max(b1_y1, b2_y1)\n",
    "        x2 = torch.min(b1_x2, b2_x2)\n",
    "        y2 = torch.min(b1_y2, b2_y2)\n",
    "        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n",
    "        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n",
    "        union = box1_area + box2_area - intersection\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        iou_score = intersection / (union + epsilon)\n",
    "\n",
    "        return iou_score\n",
    "    else:\n",
    "        # Intersection Area\n",
    "        intersection_area = torch.min(box1[..., 0], box2[..., 0]) * torch.min(\n",
    "            box1[..., 1], box2[..., 1]\n",
    "        )\n",
    "\n",
    "        box1_area = box1[..., 0] * box1[..., 1]\n",
    "        box2_area = box2[..., 0] * box2[..., 1]\n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "        iou_score = intersection_area / (union_area + 1e-6)\n",
    "        return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bboxes, iou_threshold, confidence_threshold):\n",
    "    # Filter by confidence\n",
    "    bboxes = [box for box in bboxes if box[1] > confidence_threshold]\n",
    "\n",
    "    # Sort by confidence (descending)\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    boxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        # Select top-confidence box\n",
    "        selected_box = bboxes.pop(0)\n",
    "        boxes_after_nms.append(selected_box)\n",
    "\n",
    "        # Compare with remaining boxes\n",
    "        remaining_boxes = []\n",
    "        for box in bboxes:\n",
    "            # Calculate IoU between selected box and current box\n",
    "            iou_score = iou(\n",
    "                torch.tensor(selected_box[2:]), torch.tensor(box[2:]), is_pred=True\n",
    "            )\n",
    "\n",
    "            # Keep only boxes with IoU < threshold\n",
    "            if iou_score < iou_threshold:\n",
    "                remaining_boxes.append(box)\n",
    "\n",
    "        bboxes = remaining_boxes\n",
    "\n",
    "    return boxes_after_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cells_to_bboxes(predictions, anchors, s, is_predictions=True):\n",
    "    batch_size = predictions.shape[0]\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    box_predictions = predictions[..., 1:5]\n",
    "\n",
    "    if is_predictions:\n",
    "        objectness = torch.sigmoid(predictions[..., 0:1])\n",
    "        class_probs = torch.softmax(predictions[..., 5:], dim=-1)\n",
    "\n",
    "        best_class_prob, best_class = torch.max(class_probs, dim=-1)\n",
    "        best_class_prob = torch.unsqueeze(-1).float()\n",
    "\n",
    "        scores = objectness * best_class_prob.unsqueeze(-1)\n",
    "\n",
    "        anchors = anchors.reshape(1, num_anchors, 1, 1, 2).to(predictions.device)\n",
    "\n",
    "        box_predictions_xy = torch.sigmoid(box_predictions[..., 0:2])\n",
    "        box_predictions_wh = torch.exp(box_predictions[..., 2:4]) * anchors\n",
    "\n",
    "        decoded_box_preds = torch.cat((box_predictions_xy, box_predictions_wh), dim=-1)\n",
    "\n",
    "    else:\n",
    "        scores = predictions[..., 0:1]\n",
    "        best_class = predictions[..., 5:6]\n",
    "        decoded_box_preds = box_predictions\n",
    "\n",
    "    grid_y_indices = (\n",
    "        torch.arange(s)\n",
    "        .repeat(batch_size, num_anchors, s, 1)\n",
    "        .unsqueeze(-1)\n",
    "        .to(predictions.device)\n",
    "    )\n",
    "    grid_x_indices = (\n",
    "        torch.arange(s)\n",
    "        .repeat(batch_size, num_anchors, 1, s)\n",
    "        .unsqueeze(-1)\n",
    "        .to(predictions.device)\n",
    "    )\n",
    "\n",
    "    x = (decoded_box_preds[..., 0:1] + grid_x_indices) / s\n",
    "    y = (decoded_box_preds[..., 1:2] + grid_y_indices) / s\n",
    "    width_height = decoded_box_preds[..., 2:4] / s\n",
    "\n",
    "    converted_bboxes = torch.cat(\n",
    "        (best_class, scores, x, y, width_height), dim=-1\n",
    "    ).reshape(batch_size, num_anchors * s * s, 6)\n",
    "\n",
    "    return converted_bboxes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "checkpoint_file = \"my_checkpoint.pth.tar\"\n",
    "\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],#[13x13]\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],#[26x26]\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],#[54x54]\n",
    "]\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "epochs = 20\n",
    "image_size = 416\n",
    "s = [image_size // 32, image_size // 16, image_size // 8]\n",
    "\n",
    "class_labels = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_image_with_ground_truth(\n",
    "    image_tensor, targets_tuple, anchors, grid_sizes, class_labels\n",
    "):\n",
    "    img_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img_np = np.clip(img_np, 0.0, 1.0)\n",
    "\n",
    "    color_map = plt.get_cmap(\"tab20b\")\n",
    "    colors = [color_map(i) for i in np.linspace(0, 1, len(class_labels))]\n",
    "\n",
    "    h, w, _ = img_np.shape\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(img_np)\n",
    "\n",
    "    for scale_idx, target in enumerate(targets_tuple):\n",
    "        S = grid_sizes[scale_idx]\n",
    "        target = target.cpu()\n",
    "        obj_indices = torch.nonzero(target[..., 0] == 1, as_tuple=False)\n",
    "\n",
    "        for anchor_on_scale, i, j in obj_indices:\n",
    "            box_data = target[anchor_on_scale, i, j, :]\n",
    "            x_cell, y_cell, w_cell, h_cell, class_id = box_data[1:]\n",
    "\n",
    "            img_rel_x = (j.float() + x_cell) / S\n",
    "            img_rel_y = (i.float() + y_cell) / S\n",
    "            img_rel_w = w_cell / S\n",
    "            img_rel_h = h_cell / S\n",
    "\n",
    "            upper_left_x = img_rel_x - img_rel_w / 2\n",
    "            upper_left_y = img_rel_y - img_rel_h / 2\n",
    "\n",
    "            try:\n",
    "                cls_int = int(class_id)\n",
    "                if cls_int >= len(colors):\n",
    "                    print(\n",
    "                        f\"Warning: class_id {cls_int} out of bounds for colors list (len={len(colors)}). Using color 0.\"\n",
    "                    )\n",
    "                    cls_int = 0\n",
    "\n",
    "                rect = patches.Rectangle(\n",
    "                    (upper_left_x * w, upper_left_y * h),\n",
    "                    img_rel_w * w,\n",
    "                    img_rel_h * h,\n",
    "                    linewidth=2,\n",
    "                    edgecolor=colors[cls_int],\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                if cls_int < len(class_labels):\n",
    "                    ax.text(\n",
    "                        upper_left_x * w,\n",
    "                        upper_left_y * h - 5,\n",
    "                        s=class_labels[cls_int],\n",
    "                        color=\"white\",\n",
    "                        verticalalignment=\"bottom\",\n",
    "                        bbox={\"color\": colors[cls_int], \"pad\": 0},\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: class_id {cls_int} out of bounds for class_labels list (len={len(class_labels)}).\"\n",
    "                    )\n",
    "\n",
    "            except IndexError:\n",
    "                print(f\"Error accessing color/label for class_id: {class_id}\")\n",
    "                continue\n",
    "            except ValueError:\n",
    "                print(f\"Error converting class_id to int: {class_id}\")\n",
    "                continue\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model,optimizer,file_name=\"my_checkpoint.pth\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, file_name)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file,model,optimizer,lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "    for param in optimizer.param_groups:\n",
    "        param[\"lr\"] = lr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416,\n",
    "        grid_sizes=[13, 26, 52],\n",
    "        num_classes=20,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.grid_sizes = grid_sizes\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.num_classes = num_classes\n",
    "        self.ignore_iou_threshold = 0.5\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(class_labels)}\n",
    "\n",
    "        label_files = [\n",
    "            f for f in os.listdir(self.label_dir) if f.lower().endswith(\".xml\")\n",
    "        ]\n",
    "        self.samples = []\n",
    "\n",
    "        for xml_files in label_files:\n",
    "            base_name = xml_files.split(\".\")[0]\n",
    "            image_path = os.path.join(self.image_dir, base_name + \".jpg\")\n",
    "            xml_path = os.path.join(self.label_dir, base_name + \".xml\")\n",
    "\n",
    "            self.samples.append((image_path, xml_path))\n",
    "\n",
    "        self.num_samples = len(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.num_samples:\n",
    "            raise IndexError(\n",
    "                f\"Index {idx} out of bounds for dataset with size {self.num_samples}\"\n",
    "            )\n",
    "\n",
    "        img_path, xml_path = self.samples[idx]\n",
    "\n",
    "        bboxes = []\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            size_ele = root.find(\"size\")\n",
    "            img_width = float(size_ele.find(\"width\").text)\n",
    "            img_height = float(size_ele.find(\"height\").text)\n",
    "\n",
    "            for obj in root.findall(\"object\"):\n",
    "                class_name = obj.find(\"name\").text\n",
    "\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    # Ignoring it cos idc\n",
    "                    continue\n",
    "                \n",
    "                class_id = self.class_to_idx[class_name]\n",
    "                bndbox = obj.find(\"bndbox\")\n",
    "                x_min = float(bndbox.find(\"xmin\").text)\n",
    "                y_min = float(bndbox.find(\"ymin\").text)\n",
    "                x_max = float(bndbox.find(\"xmax\").text)\n",
    "                y_max = float(bndbox.find(\"ymax\").text)\n",
    "\n",
    "                xmin = max(0, x_min)\n",
    "                ymin = max(0, y_min)\n",
    "                xmax = min(img_width, x_max)\n",
    "                ymax = min(img_height, y_max)\n",
    "\n",
    "                box_width = xmax - xmin\n",
    "                box_height = ymax - ymin\n",
    "\n",
    "                center_x = (xmin + xmax) / 2\n",
    "                center_y = (ymin + ymax) / 2\n",
    "                norm_center_x = center_x / img_width\n",
    "                norm_center_y = center_y / img_height\n",
    "                norm_width = box_width / img_width\n",
    "                norm_height = box_height / img_height\n",
    "\n",
    "                bboxes.append(\n",
    "                    [\n",
    "                        norm_center_x,\n",
    "                        norm_center_y,\n",
    "                        norm_width,\n",
    "                        norm_height,\n",
    "                        class_id,\n",
    "                    ]\n",
    "                )\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error parsing XML file: {xml_path}\")\n",
    "            raise FileNotFoundError(\"Annotation file not found: {xml_path}\")\n",
    "\n",
    "        try:\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error loading image file: {img_path}\")\n",
    "            raise FileNotFoundError(\"Image file not found: {img_path}\")\n",
    "\n",
    "        if self.transform:\n",
    "            try:\n",
    "                augs = self.transform(image=image, bboxes=bboxes)\n",
    "                image = augs[\"image\"]\n",
    "                bboxes = augs[\"bboxes\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error during transformations for the image: {img_path}\")\n",
    "                raise e\n",
    "\n",
    "        # Create target tensor\n",
    "        targets = [\n",
    "            torch.zeros((self.num_anchors_per_scale, s, s, 6)) for s in self.grid_sizes\n",
    "        ]\n",
    "\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou(\n",
    "                torch.tensor(box[2:4]),\n",
    "                self.anchors,\n",
    "                is_pred=False,\n",
    "            )\n",
    "\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "\n",
    "            if width <= 0 or height <= 0:\n",
    "                continue\n",
    "\n",
    "            has_anchor = [False] * 3\n",
    "\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                s = self.grid_sizes[scale_idx]\n",
    "                # Prevent potential index out of bounds from bad coordinates after transform\n",
    "                i, j = min(s - 1, int(s * y)), min(s - 1, int(s * x))\n",
    "\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = s * x - j, s * y - i\n",
    "                    width_cell, height_cell = (width * s, height * s)\n",
    "\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif (\n",
    "                    not anchor_taken\n",
    "                    and iou_anchors[anchor_idx] > self.ignore_iou_threshold\n",
    "                ):\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\n",
    "        return image, tuple(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5754/3353302302.py:4: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(\n",
      "/tmp/ipykernel_5754/3353302302.py:21: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n",
      "  A.PadIfNeeded(\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=image_size),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=image_size,\n",
    "            min_width=image_size,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=[0, 0, 0], \n",
    "        ),\n",
    "        A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")\n",
    "\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=image_size),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=image_size,\n",
    "            min_width=image_size,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=[0, 0, 0],  \n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'VOCdevkit/VOC2007/Annotations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m IMAGE_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVOCdevkit/VOC2007/JPEGImages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m LABEL_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVOCdevkit/VOC2007/Annotations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mObjectDetectionDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMAGE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLABEL_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43manchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mANCHORS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     12\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     13\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m image_tensor, targets_tuple \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mObjectDetectionDataset.__init__\u001b[0;34m(self, image_dir, label_dir, anchors, image_size, grid_sizes, num_classes, transform)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_iou_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mcls\u001b[39m: i \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(class_labels)}\n\u001b[1;32m     24\u001b[0m label_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 25\u001b[0m     f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xml_files \u001b[38;5;129;01min\u001b[39;00m label_files:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'VOCdevkit/VOC2007/Annotations'"
     ]
    }
   ],
   "source": [
    "IMAGE_DIR = \"VOCdevkit/VOC2007/JPEGImages\"\n",
    "LABEL_DIR = \"VOCdevkit/VOC2007/Annotations\"\n",
    "\n",
    "train_dataset = ObjectDetectionDataset(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    label_dir=LABEL_DIR,\n",
    "    anchors=ANCHORS, \n",
    "    transform=train_transform,  \n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4, \n",
    "    pin_memory=True,\n",
    ")\n",
    "image_tensor, targets_tuple = train_dataset[0]\n",
    "plot_image_with_ground_truth(image_tensor, targets_tuple, ANCHORS, s, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_batch_norm=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels, out_channels, bias=not use_batch_norm, **kwargs\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.bn(x)\n",
    "            return self.activation(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "\n",
    "        res_layers = []\n",
    "        for _ in range(num_repeats):\n",
    "            res_layers += [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(channels, channels // 2, kernel_size=1),\n",
    "                    nn.BatchNorm2d(channels // 2),\n",
    "                    nn.LeakyReLU(0.1),\n",
    "                    nn.Conv2d(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm2d(channels),\n",
    "                    nn.LeakyReLU(0.1),\n",
    "                )\n",
    "            ]\n",
    "        self.layers = nn.ModuleList(res_layers)\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            residual = x\n",
    "            x = layer(x)\n",
    "            if self.use_residual:\n",
    "                x = x + residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2 * in_channels),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(2 * in_channels, (num_classes + 5) * 3, kernel_size=1),\n",
    "        )  \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pred(x)\n",
    "        output = output.view(x.size(0), 3, self.num_classes + 5, x.size(2), x.size(3))\n",
    "        output = output.permute(0, 1, 3, 4, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                CNNBlock(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "                CNNBlock(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "                ResidualBlock(64, num_repeats=1),\n",
    "                CNNBlock(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "                ResidualBlock(128, num_repeats=2),\n",
    "                CNNBlock(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "                ResidualBlock(256, num_repeats=8),\n",
    "                CNNBlock(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "                ResidualBlock(512, num_repeats=8),\n",
    "                CNNBlock(512, 1024, kernel_size=3, stride=2, padding=1),\n",
    "                ResidualBlock(1024, num_repeats=4),\n",
    "                CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "                CNNBlock(512, 1024, kernel_size=3, stride=1, padding=1),\n",
    "                ResidualBlock(1024, use_residual=False, num_repeats=1),\n",
    "                CNNBlock(1024, 512, kernel_size=1, stride=1, padding=0),\n",
    "                ScalePrediction(512, num_classes=num_classes),\n",
    "                CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                CNNBlock(768, 256, kernel_size=1, stride=1, padding=0),\n",
    "                CNNBlock(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "                ResidualBlock(512, use_residual=False, num_repeats=1),\n",
    "                CNNBlock(512, 256, kernel_size=1, stride=1, padding=0),\n",
    "                ScalePrediction(256, num_classes=num_classes),\n",
    "                CNNBlock(256, 128, kernel_size=1, stride=1, padding=0),\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                CNNBlock(384, 128, kernel_size=1, stride=1, padding=0),\n",
    "                CNNBlock(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                ResidualBlock(256, use_residual=False, num_repeats=1),\n",
    "                CNNBlock(256, 128, kernel_size=1, stride=1, padding=0),\n",
    "                ScalePrediction(128, num_classes=num_classes),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        route_connections = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,pred,target,anchors):\n",
    "        obj = target[...,0] == 1\n",
    "        no_obj = target[...,0] == 0\n",
    "\n",
    "        no_object_loss = self.bce(\n",
    "            (pred[...,0:1][no_obj]), (target[...,0:1][no_obj])\n",
    "        )\n",
    "        anchors = anchors.reshape(1,3,1,1,2)\n",
    "        box_preds = torch.cat([\n",
    "            self.sigmoid(pred[...,1:3]),\n",
    "            torch.exp(pred[...,3:5]) * anchors\n",
    "        ],dim=-1)\n",
    "\n",
    "        ious = iou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        object_loss = self.mse(self.sigmoid(pred[...,0:1][obj]),\n",
    "                               ious*target[...,0:1][obj])\n",
    "        pred[...,1:3] = self.sigmoid(pred[...,1:3])\n",
    "\n",
    "        target[...,3:5] = torch.log(1e-6 + target[...,3:5] / anchors)\n",
    "\n",
    "        box_loss = self.mse(pred[..., 1:5][obj], \n",
    "                            target[..., 1:5][obj]) \n",
    "\n",
    "        class_loss = self.cross_entropy(\n",
    "            (pred[..., 5:][obj]), target[..., 5][obj].long()\n",
    "        )\n",
    "        \n",
    "        return ( \n",
    "            box_loss \n",
    "            + object_loss \n",
    "            + no_object_loss \n",
    "            + class_loss \n",
    "        )      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "IMAGE_EXT = \".jpg\"\n",
    "CHECKPOINT_FILE = \"yolo_voc_best_loss.pth.tar\"  \n",
    "FINAL_MODEL_FILE = \"yolo_voc_final.pth.tar\"\n",
    "\n",
    "CONFIDENCE_THRESHOLD = 0.1\n",
    "NMS_IOU_THRESH = 0.5\n",
    "MAP_IOU_THRESH = 0.5\n",
    "EVAL_EPOCH_FREQ = 5\n",
    "\n",
    "WANDB_PROJECT = \"YOLOv3-VOC-Scratch-Eval\"\n",
    "WANDB_ENTITY = None\n",
    "WANDB_RUN_NAME = f\"run_{int(time.time())}\"\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "EARLY_STOPPING_MIN_DELTA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(\n",
    "    image_dir,\n",
    "    label_dir,\n",
    "    anchors,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    val_split=0.1,\n",
    "):\n",
    "    \"\"\"Creates training and validation DataLoaders.\"\"\"\n",
    "\n",
    "    print(\"Creating datasets...\")\n",
    "    try:\n",
    "        dataset_for_split = ObjectDetectionDataset(\n",
    "            image_dir=image_dir,\n",
    "            label_dir=label_dir,\n",
    "            anchors=anchors,\n",
    "            transform=None,\n",
    "        )\n",
    "\n",
    "        total_size = len(dataset_for_split)\n",
    "        val_size = int(total_size * val_split)\n",
    "        train_size = total_size - val_size\n",
    "\n",
    "        train_indices, val_indices = random_split(\n",
    "            range(total_size), [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        train_dataset_tf = ObjectDetectionDataset(\n",
    "            image_dir=image_dir,\n",
    "            label_dir=label_dir,\n",
    "            anchors=anchors,\n",
    "            transform=train_transform,\n",
    "        )\n",
    "        val_dataset_tf = ObjectDetectionDataset(\n",
    "            image_dir=image_dir,\n",
    "            label_dir=label_dir,\n",
    "            anchors=anchors,\n",
    "            transform=val_transform,\n",
    "        )\n",
    "\n",
    "        train_dataset_final = torch.utils.data.Subset(\n",
    "            train_dataset_tf, train_indices.indices\n",
    "        )\n",
    "        val_dataset_final = torch.utils.data.Subset(val_dataset_tf, val_indices.indices)\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset_final,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset_final,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, scaler, scaled_anchors, epoch, device):\n",
    "    model.train()\n",
    "    loop = tqdm(loader, leave=True, desc=f\"Epoch {epoch+1} Train\")\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(device)\n",
    "        y0, y1, y2 = (y[0].to(device), y[1].to(device), y[2].to(device))\n",
    "        targets = [y0, y1, y2]\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(x)\n",
    "            total_loss = (\n",
    "                loss_fn(outputs[0], targets[0], scaled_anchors[0])\n",
    "                + loss_fn(outputs[1], targets[1], scaled_anchors[1])\n",
    "                + loss_fn(outputs[2], targets[2], scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        current_loss = total_loss.item()\n",
    "        losses.append(current_loss)\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)\n",
    "\n",
    "    avg_epoch_loss = sum(losses) / len(losses)\n",
    "    print(f\"\\nEpoch {epoch+1} Average Training Loss: {avg_epoch_loss:.4f}\")\n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "def validate_fn(loader, model, loss_fn, scaled_anchors, epoch, device):\n",
    "    model.eval()\n",
    "    loop = tqdm(loader, leave=True, desc=f\"Epoch {epoch+1} Validate Loss\")\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y) in enumerate(loop):\n",
    "            x = x.to(device)\n",
    "            y0, y1, y2 = (y[0].to(device), y[1].to(device), y[2].to(device))\n",
    "            targets = [y0, y1, y2]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(x)\n",
    "                total_loss = (\n",
    "                    loss_fn(outputs[0], targets[0], scaled_anchors[0])\n",
    "                    + loss_fn(outputs[1], targets[1], scaled_anchors[1])\n",
    "                    + loss_fn(outputs[2], targets[2], scaled_anchors[2])\n",
    "                )\n",
    "\n",
    "            current_loss = total_loss.item()\n",
    "            losses.append(current_loss)\n",
    "            mean_loss = sum(losses) / len(losses)\n",
    "            loop.set_postfix(val_loss=mean_loss)\n",
    "\n",
    "    avg_epoch_loss = sum(losses) / len(losses)\n",
    "    print(f\"Epoch {epoch+1} Average Validation Loss: {avg_epoch_loss:.4f}\")\n",
    "    model.train()\n",
    "    return avg_epoch_loss\n",
    "\n",
    "\n",
    "def get_evaluation_bboxes(\n",
    "    loader,\n",
    "    model,\n",
    "    iou_threshold,\n",
    "    confidence_threshold,\n",
    "    anchors,\n",
    "    device,\n",
    "    grid_sizes=s,\n",
    "):\n",
    "    \"\"\"Gets model predictions and ground truths for mAP calculation.\"\"\"\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "\n",
    "    loop = tqdm(loader, desc=\"  Calculating mAP: Inference\", leave=False)\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        bboxes = [[] for _ in range(batch_size)]\n",
    "\n",
    "        for i in range(3):\n",
    "            S = predictions[i].shape[2]\n",
    "            anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
    "\n",
    "            boxes_scale_i = convert_cells_to_bboxes(\n",
    "                predictions[i], anchor, s=S, is_predictions=True\n",
    "            )\n",
    "            for idx, (box) in enumerate(boxes_scale_i):\n",
    "                bboxes[idx].extend(box)\n",
    "\n",
    "        true_bboxes = [[] for _ in range(batch_size)]\n",
    "        targets = y\n",
    "        for i in range(3):\n",
    "            S = targets[i].shape[2]\n",
    "\n",
    "            obj_indices = torch.nonzero(targets[i][..., 0] == 1, as_tuple=True)\n",
    "\n",
    "            for batch_i, anchor_on_scale, grid_y, grid_x in zip(*obj_indices):\n",
    "                target_box_data = targets[i][\n",
    "                    batch_i, anchor_on_scale, grid_y, grid_x, :\n",
    "                ]\n",
    "                p_obj, x_cell, y_cell, w_cell, h_cell, class_id = target_box_data\n",
    "\n",
    "                img_rel_x = (grid_x.float() + x_cell) / S\n",
    "                img_rel_y = (grid_y.float() + y_cell) / S\n",
    "                img_rel_w = w_cell / S\n",
    "                img_rel_h = h_cell / S\n",
    "\n",
    "                true_bboxes[batch_i].append(\n",
    "                    [\n",
    "                        class_id.item(),\n",
    "                        img_rel_x.item(),\n",
    "                        img_rel_y.item(),\n",
    "                        img_rel_w.item(),\n",
    "                        img_rel_h.item(),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "\n",
    "            nms_boxes = nms(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                confidence_threshold=confidence_threshold,\n",
    "            )\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "\n",
    "                if box[3] > 0 and box[4] > 0:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "def calculate_map(pred_boxes, true_boxes, iou_threshold, num_classes, class_labels):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision (mAP)\n",
    "    Parameters:\n",
    "        pred_boxes (list): [[train_idx, class_pred, prob_score, x, y, w, h], ...]\n",
    "        true_boxes (list): [[train_idx, class_true, x, y, w, h], ...]\n",
    "        iou_threshold (float): IoU threshold for matching predictions to GT\n",
    "        num_classes (int): Number of classes\n",
    "        class_labels (list): List of class names for printing results\n",
    "    Returns:\n",
    "        float: mAP value across all classes\n",
    "        dict: AP value per class\n",
    "    \"\"\"\n",
    "    average_precisions = {}\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    true_boxes_dict = {}\n",
    "    for tb in true_boxes:\n",
    "        img_idx = tb[0]\n",
    "        if img_idx not in true_boxes_dict:\n",
    "            true_boxes_dict[img_idx] = []\n",
    "        true_boxes_dict[img_idx].append(tb)\n",
    "\n",
    "    print(\"  Calculating mAP: Matching and AP calculation\")\n",
    "    for c in range(num_classes):\n",
    "        predictions = [pb for pb in pred_boxes if pb[1] == c]\n",
    "        ground_truths = [tb for tb in true_boxes if tb[1] == c]\n",
    "\n",
    "        num_gt = len(ground_truths)\n",
    "        if num_gt == 0:\n",
    "            average_precisions[c] = 0.0\n",
    "            continue\n",
    "\n",
    "        amount_bboxes = {}\n",
    "        for tb in ground_truths:\n",
    "            img_idx = tb[0]\n",
    "            if img_idx not in amount_bboxes:\n",
    "                amount_bboxes[img_idx] = {\"num_boxes\": 0, \"detected\": []}\n",
    "            amount_bboxes[img_idx][\"num_boxes\"] += 1\n",
    "            amount_bboxes[img_idx][\"detected\"].append(False)\n",
    "\n",
    "        predictions.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros(len(predictions))\n",
    "        FP = torch.zeros(len(predictions))\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            average_precisions[c] = 0.0\n",
    "            continue\n",
    "\n",
    "        for detection_idx, pred in enumerate(predictions):\n",
    "            pred_img_idx = pred[0]\n",
    "\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in true_boxes_dict.get(pred_img_idx, []) if bbox[1] == c\n",
    "            ]\n",
    "\n",
    "            if not ground_truth_img:\n",
    "                FP[detection_idx] = 1\n",
    "                continue\n",
    "\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "\n",
    "            for gt_idx, gt in enumerate(ground_truth_img):\n",
    "\n",
    "                iou_score = iou(\n",
    "                    torch.tensor(pred[3:]),\n",
    "                    torch.tensor(gt[2:]),\n",
    "                    is_pred=True,\n",
    "                )\n",
    "\n",
    "                if iou_score > best_iou:\n",
    "                    best_iou = iou_score\n",
    "                    best_gt_idx = gt_idx\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                gt_img_details = amount_bboxes[pred_img_idx]\n",
    "\n",
    "                if (\n",
    "                    best_gt_idx < len(gt_img_details[\"detected\"])\n",
    "                    and not gt_img_details[\"detected\"][best_gt_idx]\n",
    "                ):\n",
    "                    TP[detection_idx] = 1\n",
    "                    gt_img_details[\"detected\"][best_gt_idx] = True\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (num_gt + epsilon)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
    "\n",
    "        precisions = torch.cat((torch.tensor([1.0]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0.0]), recalls))\n",
    "\n",
    "        for i in range(len(precisions) - 2, -1, -1):\n",
    "            precisions[i] = torch.max(precisions[i], precisions[i + 1])\n",
    "\n",
    "        recall_points = torch.linspace(0, 1, 11)\n",
    "        ap = 0.0\n",
    "        for point in recall_points:\n",
    "            try:\n",
    "\n",
    "                idx = torch.where(recalls >= point)[0][-1]\n",
    "                ap += precisions[idx]\n",
    "            except IndexError:\n",
    "                ap += 0.0\n",
    "        ap /= 11.0\n",
    "        average_precisions[c] = ap.item()\n",
    "\n",
    "    mAP = (\n",
    "        sum(average_precisions.values()) / len(average_precisions)\n",
    "        if average_precisions\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- AP per class ---\")\n",
    "    for c, ap_val in average_precisions.items():\n",
    "        class_name = class_labels[c] if c < len(class_labels) else f\"Class {c}\"\n",
    "        print(f\"{class_name:<15}: {ap_val:.4f}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "    return mAP, average_precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3750/1544322500.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/conda/envs/idm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD_MODEL is False or checkpoint not found. Starting from scratch.\n",
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train:   0%|          | 0/561 [00:00<?, ?it/s]/tmp/ipykernel_3750/1045812725.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1 Train: 100%|| 561/561 [02:00<00:00,  4.66it/s, loss=10.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Average Training Loss: 10.1773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validate Loss:   0%|          | 0/63 [00:00<?, ?it/s]/tmp/ipykernel_3750/1045812725.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1 Validate Loss: 100%|| 63/63 [00:04<00:00, 14.89it/s, val_loss=8.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Validation Loss: 8.0094\n",
      "=> Saving checkpoint\n",
      "--> Val loss improved to 8.0094. Saved best model checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Train:   6%|         | 35/561 [00:07<01:58,  4.43it/s, loss=7.74]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 218\u001b[0m\n\u001b[1;32m    216\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m last_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m---> 77\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaled_anchors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val_loader) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m validate_fn(\n\u001b[1;32m     90\u001b[0m             val_loader, model, loss_fn, scaled_anchors, epoch, device\n\u001b[1;32m     91\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[24], line 24\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, scaled_anchors, epoch, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     22\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 24\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(current_loss)\n\u001b[1;32m     26\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(losses) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(losses)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x711c337b9f30>> (for post_run_cell), with arguments args (<ExecutionResult object at 711dc842a1d0, execution_count=26 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 711dc842b4f0, raw_cell=\"def main():\n",
      "    global LOAD_MODEL\n",
      "    wandb.init(\n",
      "..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2Blearn-g5-xl/home/ubuntu/oracle/learn_cv/yolo_scratch/object_detection.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:555\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:771\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:368\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 368\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:47\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:222\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    220\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[1;32m    221\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:154\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:151\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/idm/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global LOAD_MODEL\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        name=WANDB_RUN_NAME,\n",
    "        config={\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"weight_decay\": WEIGHT_DECAY,\n",
    "            \"architecture\": \"YOLOv3\",\n",
    "            \"dataset\": \"PASCAL_VOC\",\n",
    "            \"image_size\": 416,\n",
    "            \"early_stopping_patience\": EARLY_STOPPING_PATIENCE,\n",
    "            \"early_stopping_min_delta\": EARLY_STOPPING_MIN_DELTA,\n",
    "            \"conf_threshold_nms\": CONFIDENCE_THRESHOLD,\n",
    "            \"nms_iou_thresh\": NMS_IOU_THRESH,\n",
    "            \"map_iou_thresh\": MAP_IOU_THRESH,\n",
    "            \"eval_epoch_freq\": EVAL_EPOCH_FREQ,\n",
    "        },\n",
    "    )\n",
    "    config = wandb.config\n",
    "\n",
    "    model = YOLOv3(num_classes=len(class_labels)).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay\n",
    "    )\n",
    "    loss_fn = YOLOLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.1, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL and os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            load_checkpoint(CHECKPOINT_FILE, model, optimizer, config.learning_rate)\n",
    "            print(f\"Loaded checkpoint '{CHECKPOINT_FILE}'\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error loading checkpoint {CHECKPOINT_FILE}: {e}. Starting from scratch.\"\n",
    "            )\n",
    "            LOAD_MODEL = False  \n",
    "    else:\n",
    "        print(\"LOAD_MODEL is False or checkpoint not found. Starting from scratch.\")\n",
    "\n",
    "    try:\n",
    "        train_loader, val_loader = get_loaders(\n",
    "            image_dir=IMAGE_DIR,\n",
    "            label_dir=LABEL_DIR,\n",
    "            anchors=ANCHORS,\n",
    "            batch_size=config.batch_size,\n",
    "            train_transform=train_transform,\n",
    "            val_transform=test_transform,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create dataloaders: {e}\")\n",
    "        wandb.finish(exit_code=1)\n",
    "        return\n",
    "\n",
    "    if len(val_loader) == 0:\n",
    "        print(\n",
    "            \"Warning: Validation loader is empty. Skipping validation, mAP calculation, and early stopping.\"\n",
    "        )\n",
    "\n",
    "    scaled_anchors = torch.tensor(ANCHORS).to(device)\n",
    "\n",
    "    wandb.watch(model, log=\"gradients\", log_freq=200)  \n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    last_map = 0.0  \n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        train_loss = train_fn(\n",
    "            train_loader,\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            scaler,\n",
    "            scaled_anchors,\n",
    "            epoch,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        if len(val_loader) > 0:\n",
    "            val_loss = validate_fn(\n",
    "                val_loader, model, loss_fn, scaled_anchors, epoch, device\n",
    "            )\n",
    "            scheduler.step(val_loss)  \n",
    "\n",
    "            log_dict = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "            }\n",
    "\n",
    "            run_map_calc = ((epoch + 1) % config.eval_epoch_freq == 0) or (\n",
    "                epoch + 1 == config.epochs\n",
    "            )\n",
    "            if run_map_calc:\n",
    "                print(f\"\\n--- Calculating mAP @ {config.map_iou_thresh} IoU ---\")\n",
    "                all_pred_boxes, all_true_boxes = get_evaluation_bboxes(\n",
    "                    val_loader,\n",
    "                    model,\n",
    "                    config.nms_iou_thresh,\n",
    "                    config.conf_threshold_nms,\n",
    "                    ANCHORS,\n",
    "                    device,\n",
    "                    s,\n",
    "                )\n",
    "\n",
    "                if not all_pred_boxes:\n",
    "                    print(\n",
    "                        \"Warning: No prediction boxes found after NMS. Skipping mAP calculation.\"\n",
    "                    )\n",
    "                    map_score = 0.0\n",
    "                    ap_per_class = {}\n",
    "                elif not all_true_boxes:\n",
    "                    print(\n",
    "                        \"Warning: No ground truth boxes found in validation set. Skipping mAP calculation.\"\n",
    "                    )\n",
    "                    map_score = 0.0\n",
    "                    ap_per_class = {}\n",
    "                else:\n",
    "                    map_score, ap_per_class = calculate_map(\n",
    "                        all_pred_boxes,\n",
    "                        all_true_boxes,\n",
    "                        config.map_iou_thresh,\n",
    "                        len(class_labels),\n",
    "                        class_labels,\n",
    "                    )\n",
    "                print(f\"--- mAP @ {config.map_iou_thresh:.2f} = {map_score:.4f} ---\")\n",
    "                log_dict[f\"mAP@{config.map_iou_thresh}\"] = map_score\n",
    "                last_map = map_score  \n",
    "\n",
    "            wandb.log(log_dict)  \n",
    "\n",
    "            is_improvement = (\n",
    "                best_val_loss - val_loss\n",
    "            ) > config.early_stopping_min_delta\n",
    "            if is_improvement and SAVE_MODEL:\n",
    "                best_val_loss = val_loss\n",
    "                save_checkpoint(model, optimizer, file_name=CHECKPOINT_FILE)\n",
    "                print(\n",
    "                    f\"--> Val loss improved to {best_val_loss:.4f}. Saved best model checkpoint.\"\n",
    "                )\n",
    "                wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "                wandb.run.summary[\"mAP_at_best_loss\"] = (\n",
    "                    last_map  \n",
    "                )\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(\n",
    "                    f\"Val loss did not improve significantly. Patience: {patience_counter}/{config.early_stopping_patience}\"\n",
    "                )\n",
    "\n",
    "            if patience_counter >= config.early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "        else:  \n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "\n",
    "        save_checkpoint(model, optimizer, file_name=FINAL_MODEL_FILE)\n",
    "        print(\n",
    "            f\"\\nTraining finished or stopped. Saved final model to {FINAL_MODEL_FILE}\"\n",
    "        )\n",
    "        if os.path.exists(CHECKPOINT_FILE):\n",
    "            print(f\"Best model (by validation loss) saved to {CHECKPOINT_FILE}\")\n",
    "\n",
    "    if (\n",
    "        len(val_loader) > 0 and not run_map_calc\n",
    "    ):  \n",
    "        print(f\"\\n--- Calculating Final mAP @ {config.map_iou_thresh} IoU ---\")\n",
    "        all_pred_boxes, all_true_boxes = get_evaluation_bboxes(\n",
    "            val_loader,\n",
    "            model,\n",
    "            config.nms_iou_thresh,\n",
    "            config.conf_threshold_nms,\n",
    "            ANCHORS,\n",
    "            device,\n",
    "            s,\n",
    "        )\n",
    "        if all_pred_boxes and all_true_boxes:\n",
    "            map_score, _ = calculate_map(\n",
    "                all_pred_boxes,\n",
    "                all_true_boxes,\n",
    "                config.map_iou_thresh,\n",
    "                len(class_labels),\n",
    "                class_labels,\n",
    "            )\n",
    "            print(f\"--- Final mAP @ {config.map_iou_thresh:.2f} = {map_score:.4f} ---\")\n",
    "            wandb.run.summary[f\"final_mAP@{config.map_iou_thresh}\"] = map_score\n",
    "        else:\n",
    "            print(\n",
    "                \"Skipping final mAP calculation due to missing predictions or ground truths.\"\n",
    "            )\n",
    "\n",
    "    wandb.finish()\n",
    "    print(\"\\nTraining complete. W&B run finished.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
